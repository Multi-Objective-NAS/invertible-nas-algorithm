{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural ODE",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSHWbX6foOl1rY44Ta4D8o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Multi-Objective-NAS/invertible-nas-algorithm/blob/master/Neural_ODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0uTqCujPcZx",
        "colab_type": "text"
      },
      "source": [
        "#Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8mLWV7lPQLL",
        "colab_type": "code",
        "outputId": "069f502f-2991-4df8-df57-37bb2fa19081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "pip install git+https://github.com/rtqichen/torchdiffeq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/rtqichen/torchdiffeq\n",
            "  Cloning https://github.com/rtqichen/torchdiffeq to /tmp/pip-req-build-7aakhgm9\n",
            "  Running command git clone -q https://github.com/rtqichen/torchdiffeq /tmp/pip-req-build-7aakhgm9\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from torchdiffeq==0.0.1) (1.4.0)\n",
            "Building wheels for collected packages: torchdiffeq\n",
            "  Building wheel for torchdiffeq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchdiffeq: filename=torchdiffeq-0.0.1-cp36-none-any.whl size=27578 sha256=3eb67328461f2324613d9b8204c6e70dd8e4ff7c7e2d16e4c08bcb598935436d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0zobjovg/wheels/3f/76/69/01867bf3355c3bc8bae7e556b17b44c395b6cda5e76fd8ddc7\n",
            "Successfully built torchdiffeq\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY6utA-AEDkP",
        "colab_type": "code",
        "outputId": "f447a29e-a522-4093-9202-20bfcd3615ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone https://github.com/rtqichen/torchdiffeq"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'torchdiffeq'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/15)\u001b[K\rremote: Counting objects:  13% (2/15)\u001b[K\rremote: Counting objects:  20% (3/15)\u001b[K\rremote: Counting objects:  26% (4/15)\u001b[K\rremote: Counting objects:  33% (5/15)\u001b[K\rremote: Counting objects:  40% (6/15)\u001b[K\rremote: Counting objects:  46% (7/15)\u001b[K\rremote: Counting objects:  53% (8/15)\u001b[K\rremote: Counting objects:  60% (9/15)\u001b[K\rremote: Counting objects:  66% (10/15)\u001b[K\rremote: Counting objects:  73% (11/15)\u001b[K\rremote: Counting objects:  80% (12/15)\u001b[K\rremote: Counting objects:  86% (13/15)\u001b[K\rremote: Counting objects:  93% (14/15)\u001b[K\rremote: Counting objects: 100% (15/15)\u001b[K\rremote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects:   7% (1/14)\u001b[K\rremote: Compressing objects:  14% (2/14)\u001b[K\rremote: Compressing objects:  21% (3/14)\u001b[K\rremote: Compressing objects:  28% (4/14)\u001b[K\rremote: Compressing objects:  35% (5/14)\u001b[K\rremote: Compressing objects:  42% (6/14)\u001b[K\rremote: Compressing objects:  50% (7/14)\u001b[K\rremote: Compressing objects:  57% (8/14)\u001b[K\rremote: Compressing objects:  64% (9/14)\u001b[K\rremote: Compressing objects:  71% (10/14)\u001b[K\rremote: Compressing objects:  78% (11/14)\u001b[K\rremote: Compressing objects:  85% (12/14)\u001b[K\rremote: Compressing objects:  92% (13/14)\u001b[K\rremote: Compressing objects: 100% (14/14)\u001b[K\rremote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "Receiving objects:   0% (1/229)   \rReceiving objects:   1% (3/229)   \rReceiving objects:   2% (5/229)   \rReceiving objects:   3% (7/229)   \rReceiving objects:   4% (10/229)   \rReceiving objects:   5% (12/229)   \rReceiving objects:   6% (14/229)   \rReceiving objects:   7% (17/229)   \rReceiving objects:   8% (19/229)   \rReceiving objects:   9% (21/229)   \rReceiving objects:  10% (23/229)   \rReceiving objects:  11% (26/229)   \rReceiving objects:  12% (28/229)   \rReceiving objects:  13% (30/229)   \rReceiving objects:  14% (33/229)   \rReceiving objects:  15% (35/229)   \rReceiving objects:  16% (37/229)   \rReceiving objects:  17% (39/229)   \rReceiving objects:  18% (42/229)   \rReceiving objects:  19% (44/229)   \rReceiving objects:  20% (46/229)   \rReceiving objects:  21% (49/229)   \rReceiving objects:  22% (51/229)   \rReceiving objects:  23% (53/229)   \rReceiving objects:  24% (55/229)   \rReceiving objects:  25% (58/229)   \rReceiving objects:  26% (60/229)   \rReceiving objects:  27% (62/229)   \rReceiving objects:  28% (65/229)   \rReceiving objects:  29% (67/229)   \rReceiving objects:  30% (69/229)   \rReceiving objects:  31% (71/229)   \rReceiving objects:  32% (74/229)   \rReceiving objects:  33% (76/229)   \rReceiving objects:  34% (78/229)   \rReceiving objects:  35% (81/229)   \rReceiving objects:  36% (83/229)   \rReceiving objects:  37% (85/229)   \rReceiving objects:  38% (88/229)   \rReceiving objects:  39% (90/229)   \rReceiving objects:  40% (92/229)   \rReceiving objects:  41% (94/229)   \rReceiving objects:  42% (97/229)   \rReceiving objects:  43% (99/229)   \rReceiving objects:  44% (101/229)   \rremote: Total 229 (delta 3), reused 4 (delta 1), pack-reused 214\u001b[K\n",
            "Receiving objects:  45% (104/229)   \rReceiving objects:  46% (106/229)   \rReceiving objects:  47% (108/229)   \rReceiving objects:  48% (110/229)   \rReceiving objects:  49% (113/229)   \rReceiving objects:  50% (115/229)   \rReceiving objects:  51% (117/229)   \rReceiving objects:  52% (120/229)   \rReceiving objects:  53% (122/229)   \rReceiving objects:  54% (124/229)   \rReceiving objects:  55% (126/229)   \rReceiving objects:  56% (129/229)   \rReceiving objects:  57% (131/229)   \rReceiving objects:  58% (133/229)   \rReceiving objects:  59% (136/229)   \rReceiving objects:  60% (138/229)   \rReceiving objects:  61% (140/229)   \rReceiving objects:  62% (142/229)   \rReceiving objects:  63% (145/229)   \rReceiving objects:  64% (147/229)   \rReceiving objects:  65% (149/229)   \rReceiving objects:  66% (152/229)   \rReceiving objects:  67% (154/229)   \rReceiving objects:  68% (156/229)   \rReceiving objects:  69% (159/229)   \rReceiving objects:  70% (161/229)   \rReceiving objects:  71% (163/229)   \rReceiving objects:  72% (165/229)   \rReceiving objects:  73% (168/229)   \rReceiving objects:  74% (170/229)   \rReceiving objects:  75% (172/229)   \rReceiving objects:  76% (175/229)   \rReceiving objects:  77% (177/229)   \rReceiving objects:  78% (179/229)   \rReceiving objects:  79% (181/229)   \rReceiving objects:  80% (184/229)   \rReceiving objects:  81% (186/229)   \rReceiving objects:  82% (188/229)   \rReceiving objects:  83% (191/229)   \rReceiving objects:  84% (193/229)   \rReceiving objects:  85% (195/229)   \rReceiving objects:  86% (197/229)   \rReceiving objects:  87% (200/229)   \rReceiving objects:  88% (202/229)   \rReceiving objects:  89% (204/229)   \rReceiving objects:  90% (207/229)   \rReceiving objects:  91% (209/229)   \rReceiving objects:  92% (211/229)   \rReceiving objects:  93% (213/229)   \rReceiving objects:  94% (216/229)   \rReceiving objects:  95% (218/229)   \rReceiving objects:  96% (220/229)   \rReceiving objects:  97% (223/229)   \rReceiving objects:  98% (225/229)   \rReceiving objects:  99% (227/229)   \rReceiving objects: 100% (229/229)   \rReceiving objects: 100% (229/229), 715.37 KiB | 5.72 MiB/s, done.\n",
            "Resolving deltas:   0% (0/100)   \rResolving deltas:   9% (9/100)   \rResolving deltas:  12% (12/100)   \rResolving deltas:  14% (14/100)   \rResolving deltas:  16% (16/100)   \rResolving deltas:  17% (17/100)   \rResolving deltas:  21% (21/100)   \rResolving deltas:  24% (24/100)   \rResolving deltas:  25% (25/100)   \rResolving deltas:  27% (27/100)   \rResolving deltas:  31% (31/100)   \rResolving deltas:  33% (33/100)   \rResolving deltas:  38% (38/100)   \rResolving deltas:  49% (49/100)   \rResolving deltas:  52% (52/100)   \rResolving deltas:  53% (53/100)   \rResolving deltas:  58% (58/100)   \rResolving deltas:  65% (65/100)   \rResolving deltas:  68% (68/100)   \rResolving deltas:  69% (69/100)   \rResolving deltas:  77% (77/100)   \rResolving deltas:  78% (78/100)   \rResolving deltas:  82% (82/100)   \rResolving deltas:  85% (85/100)   \rResolving deltas:  88% (88/100)   \rResolving deltas:  98% (98/100)   \rResolving deltas: 100% (100/100)   \rResolving deltas: 100% (100/100), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvSUB8hRR3_m",
        "colab_type": "text"
      },
      "source": [
        "#Installation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i06DFfXR22T",
        "colab_type": "code",
        "outputId": "22093d49-e1fd-4e3a-c7a8-9ee965f97a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "#!curl -O https://storage.googleapis.com/nasbench/nasbench_full.tfrecord\n",
        "!curl -O https://storage.googleapis.com/nasbench/nasbench_only108.tfrecord\n",
        "# Clone and install the code and dependencies.\n",
        "\n",
        "!git clone https://github.com/google-research/nasbench\n",
        "#!pip install ./nasbench\n",
        "\n",
        "# Initialize the NASBench object which parses the raw data into memory (this\n",
        "# should only be run once as it takes up to a few minutes)."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  498M  100  498M    0     0  88.1M      0  0:00:05  0:00:05 --:--:-- 87.2M\n",
            "Cloning into 'nasbench'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 96 (delta 0), reused 2 (delta 0), pack-reused 92\u001b[K\n",
            "Unpacking objects: 100% (96/96), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGWYcVzqR7u-",
        "colab_type": "text"
      },
      "source": [
        "#Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh6xZU7bPbmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJfb9TERKpPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchdiffeq import odeint_adjoint as odeint\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6wHSd1hJP8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Main Argument\n",
        "'''\n",
        "DEVICE = torch.device('cuda:0')\n",
        "#DEVICE = torch.device('cpu')\n",
        "BATCH_SIZE = 2\n",
        "LR = 0.1\n",
        "NEPOCHS = 1 \n",
        "RTOL = 1e-7\n",
        "ATOL = 1e-9\n",
        "EDGE_NUM = 9\n",
        "\n",
        "'''\n",
        "Data Set\n",
        "'''\n",
        "# Useful constants\n",
        "INPUT = 'input'\n",
        "OUTPUT = 'output'\n",
        "CONV3X3 = 'conv3x3-bn-relu'\n",
        "CONV1X1 = 'conv1x1-bn-relu'\n",
        "MAXPOOL3X3 = 'maxpool3x3'\n",
        "OUTPUT = 'output'\n",
        "NUM_VERTICES = 7\n",
        "MAX_EDGES = 9\n",
        "EDGE_SPOTS = NUM_VERTICES * (NUM_VERTICES - 1) / 2  # Upper triangular matrix\n",
        "OP_SPOTS = NUM_VERTICES - 2  # Input/output vertices are fixed\n",
        "ALLOWED_OPS = [CONV3X3, CONV1X1, MAXPOOL3X3, OUTPUT]\n",
        "ALLOWED_EDGES = [0, 1]  # Binary adjacency matrix\n",
        "EMBED_SIZE = 7 + 7 + len(ALLOWED_OPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GBGPrRvBqM0",
        "colab_type": "code",
        "outputId": "3c0b1ace-d83a-4c25-994b-17146e054a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "%cd nasbench\n",
        "%ls\n",
        "!pwd"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/nasbench\n",
            "CONTRIBUTING.md  \u001b[0m\u001b[01;34mimages\u001b[0m/  \u001b[01;34mnasbench\u001b[0m/       README.md\n",
            "example.py       LICENSE  NASBench.ipynb  setup.py\n",
            "/content/nasbench\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuYD8SyyTcL_",
        "colab_type": "code",
        "outputId": "4df25558-fa56-41f5-dc2d-898c944ac0b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "from nasbench import api\n",
        "#nasbench = api.NASBench('nasbench_full.tfrecord')\n",
        "\n",
        "%cd ..\n",
        "\n",
        "nasbench_api = api.NASBench('nasbench_only108.tfrecord')\n",
        "nasbench_hashkeys = nasbench_api.hash_iterator()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/nasbench/nasbench/lib/training_time.py:130: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nasbench/nasbench/lib/training_time.py:174: The name tf.train.CheckpointSaverListener is deprecated. Please use tf.estimator.CheckpointSaverListener instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nasbench/nasbench/lib/evaluate.py:30: The name tf.train.NanLossDuringTrainingError is deprecated. Please use tf.estimator.NanLossDuringTrainingError instead.\n",
            "\n",
            "/content\n",
            "Loading dataset from file... This may take a few minutes...\n",
            "WARNING:tensorflow:From /content/nasbench/nasbench/api.py:146: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "Loaded dataset in 47 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDVfj4D5SAbf",
        "colab_type": "text"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H16SkgkDTk0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(mat, ops):\n",
        "    def _sanity_check(idx1, opidx, idx2):\n",
        "        if idx1 not in range(7):\n",
        "            return False\n",
        "        if idx2 not in range(7):\n",
        "            return False\n",
        "        if opidx not in range(4):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    encoded = []\n",
        "    for inbound, outbound in zip(*np.array(mat).nonzero()):\n",
        "        op = ALLOWED_OPS.index(ops[outbound])\n",
        "        assert _sanity_check(inbound, op, outbound)\n",
        "\n",
        "        embed = [0] * EMBED_SIZE\n",
        "        embed[inbound] = 1\n",
        "        embed[7 + op] = 1\n",
        "        embed[7 + len(ALLOWED_OPS) + outbound] = 1\n",
        "\n",
        "        encoded.append(embed)\n",
        "\n",
        "    assert np.array(encoded).shape == (9, EMBED_SIZE)\n",
        "    return encoded\n",
        "\n",
        "\n",
        "def max_index(output):\n",
        "    # (1, 9,18)\n",
        "    encoded = []\n",
        "    outputs = [output_ for output_ in output[0]]\n",
        "    \n",
        "    for output in outputs:\n",
        "        embed = [0 for _ in range(EMBED_SIZE)]\n",
        "        inbound, op, outbound = np.argmax(output[:7]), np.argmax(output[7:-7]), np.argmax(output[-7:])\n",
        "        embed[inbound] = 1\n",
        "        embed[7 + op] = 1\n",
        "        embed[7 + len(ALLOWED_OPS) + outbound] = 1\n",
        "        assert np.sum(np.array(embed)) == 3\n",
        "        encoded.append(embed)\n",
        "\n",
        "    assert np.array(encoded).shape == (9, EMBED_SIZE)\n",
        "    return encoded\n",
        "\n",
        "\n",
        "def decode(encoded):\n",
        "    mat = np.zeros((7, 7))\n",
        "    ops = [INPUT] + [CONV3X3 for _ in range(5)] + [OUTPUT]\n",
        "\n",
        "    for embed in encoded:\n",
        "        inbound, op, outbound = np.nonzero(np.array(embed))[0]\n",
        "        op -= 7\n",
        "        outbound -= (7 + len(ALLOWED_OPS))\n",
        "        ops[outbound] = ALLOWED_OPS[op]\n",
        "        if inbound > outbound:\n",
        "            (inbound, outbound) = (outbound, inbound)\n",
        "        elif inbound == outbound:\n",
        "            continue\n",
        "        mat[inbound][outbound] = 1\n",
        "\n",
        "    return mat.tolist(), ops"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkpTwA027xVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert np.array([[1,2,3],[1,2,3]]).shape == (2,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxgUzP7CG4_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader(object):\n",
        "    def __init__(self, start, end, batchsize=None):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.cur = start\n",
        "        if batchsize is None:\n",
        "            self.batchsize = end - start\n",
        "        else:\n",
        "            self.batchsize = batchsize\n",
        "        self.size = end - start\n",
        "\n",
        "    def __run_experiment(self, mat, op):\n",
        "        model_spec = api.ModelSpec(matrix=mat, ops=op)\n",
        "        q = nasbench_api.query(model_spec)\n",
        "        return q['validation_accuracy'], q['training_time']\n",
        "\n",
        "    def next(self):\n",
        "        global nasbench_hashkeys\n",
        "\n",
        "        xset = []\n",
        "        yset = []\n",
        "        if self.cur == self.end:\n",
        "            self.cur = self.end\n",
        "        for idx in range(self.cur, min(self.cur + self.batchsize, self.end)):\n",
        "            hash_val = nasbench_hashkeys[idx]\n",
        "            fixed_stat, _ = nasbench_api.get_metrics_from_hash(hash_val)\n",
        "            mat, op = fixed_stat['module_adjacency'], fixed_stat['module_operations']\n",
        "            x = encode(mat, op)\n",
        "            y = self.__run_experiment(mat, op)\n",
        "            xset.append(x)\n",
        "            yset.append(y)\n",
        "        xset = torch.FloatTensor(xset)\n",
        "        yset = torch.FloatTensor(yset)\n",
        "        return xset, yset\n",
        "\n",
        "\n",
        "def train_val_test_split(batch_size=100, validation_ratio=0.1, test_ratio=0.1):\n",
        "    global nasbench_hashkeys\n",
        "\n",
        "    revised = []\n",
        "    for hash_val in nasbench_hashkeys:\n",
        "        fixed_stat, _ = nasbench_api.get_metrics_from_hash(hash_val)\n",
        "        mat, op = fixed_stat['module_adjacency'], fixed_stat['module_operations']\n",
        "        if np.sum(mat) == EDGE_NUM:\n",
        "            revised.append(hash_val)\n",
        "\n",
        "    total_size = (len(revised) // batch_size) * batch_size\n",
        "    nasbench_hashkeys = revised[: total_size]\n",
        "    val_size = int(total_size * validation_ratio)\n",
        "    test_size = int(total_size * test_ratio)\n",
        "\n",
        "    return DataLoader(val_size+test_size, total_size, batch_size), DataLoader(0, val_size), DataLoader(val_size, val_size+test_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56pJRxXyGPtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "def norm(dim):\n",
        "    return nn.GroupNorm( dim, dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r50bPVRngIPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConcatConv1d(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(ConcatConv1d, self).__init__()\n",
        "        self._layer = nn.Conv1d(\n",
        "            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        tt = torch.ones_like(x[:, :1, :]) * t\n",
        "        ttx = torch.cat([tt, x], 1)\n",
        "        return self._layer(ttx)\n",
        "\n",
        "\n",
        "class ODEfunc(nn.Module):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super(ODEfunc, self).__init__()\n",
        "        self.norm1 = norm(dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = ConcatConv1d(dim, dim, 3, 1, 1)\n",
        "        self.norm2 = norm(dim)\n",
        "        self.conv2 = ConcatConv1d(dim, dim, 3, 1, 1)\n",
        "        self.norm3 = norm(dim)\n",
        "        self.nfe = 0\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        self.nfe += 1\n",
        "        out = self.norm1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(t, out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(t, out)\n",
        "        out = self.norm3(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXc6zDI_SsoG",
        "colab_type": "code",
        "outputId": "2208ca40-ba8c-4b44-c10c-26ea739bc7e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#test\n",
        "a = torch.ones(3, 2)\n",
        "b = torch.zeros(3, 3)\n",
        "torch.cat([a, b], dim=1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_XXh0e5GdBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ODEBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, odefunc, method):\n",
        "        super(ODEBlock, self).__init__()\n",
        "        self.odefunc = odefunc\n",
        "        self.integration_time = torch.tensor([0, 1]).float()\n",
        "        self.method = method\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.integration_time = self.integration_time.type_as(x)\n",
        "        #batch_t = T_series[:x.shape[0]]\n",
        "        out = odeint(self.odefunc, x, self.integration_time , rtol=RTOL, atol=ATOL, method = self.method)\n",
        "        return out[1]\n",
        "\n",
        "    @property\n",
        "    def nfe(self):\n",
        "        return self.odefunc.nfe\n",
        "\n",
        "    @nfe.setter\n",
        "    def nfe(self, value):\n",
        "        self.odefunc.nfe = value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83nFNskiGg8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
        "        ret = x.view(-1, shape)\n",
        "        return ret\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5opgvqcGqac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RunningAverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, momentum=0.99):\n",
        "        self.momentum = momentum\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = None\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, val):\n",
        "        if self.val is None:\n",
        "            self.avg = val\n",
        "        else:\n",
        "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
        "        self.val = val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC9wuSPnHw4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n",
        "    initial_learning_rate = LR * batch_size / batch_denom\n",
        "\n",
        "    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n",
        "    vals = [initial_learning_rate * decay for decay in decay_rates]\n",
        "\n",
        "    def learning_rate_fn(itr):\n",
        "        lt = [itr < b for b in boundaries] + [True]\n",
        "        i = np.argmax(lt)\n",
        "        return vals[i]\n",
        "\n",
        "    return learning_rate_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol-Bb4xdIEp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(model, dataloader):\n",
        "    datasize = dataloader.end - dataloader.start\n",
        "    x, y = dataloader.next(datasize)\n",
        "    x = torch.FloatTensor(x)\n",
        "    x = x.to(DEVICE)\n",
        "    y = y.numpy()\n",
        "    result = model(x).cpu().detach().numpy()\n",
        "    total_correct = np.sum(np.square(result - y))\n",
        "    return total_correct / datasize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZrD_tl5JNBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLs5RezsqnAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ODEModel(nn.Module):\n",
        "    def __init__(self, dim, method=None):\n",
        "        # dim is edge number\n",
        "        super(ODEModel, self).__init__()\n",
        "        self.feature_layers = [ODEBlock(ODEfunc(dim), method)]\n",
        "        self.fc_layers = [norm(dim), nn.ReLU(inplace=True), nn.AdaptiveAvgPool1d(1), Flatten(), nn.Linear(dim, 2)]\n",
        "        self.model = nn.Sequential(*self.feature_layers, *self.fc_layers).to(DEVICE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, optimizer, criterion, accuracy, lr_fn, meter_fn):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.accuracy = accuracy\n",
        "        self.lr_fn = lr_fn\n",
        "        self.batch_time_meter = meter_fn\n",
        "        self.f_nfe_meter = meter_fn\n",
        "        self.b_nfe_meter = meter_fn\n",
        "    \n",
        "    def run(self, epochs, train_dataloader, val_dataloader, test_dataloader):\n",
        "        train_size = train_dataloader.size\n",
        "        batches_per_eoch = train_size // train_dataloader.batchsize\n",
        "        end = time.time()\n",
        "        for itr in range(epochs):\n",
        "            for batch_turn in range(batches_per_epoch):\n",
        "                x, y = train_dataloader.next()\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group['lr'] = self.lr_fn(itr * batches_per_epoch + batch_turn)\n",
        "                self.optimizer.zero_grad()\n",
        "                x = x.to(DEVICE)\n",
        "                y = y.to(DEVICE)\n",
        "                logits = self.model(x)\n",
        "                result = logits.cpu().detach().numpy()\n",
        "                loss = self.criterion(logits, y)\n",
        "                #print(\"[%d] Completed loss\" % batch_turn)\n",
        "                nfe_forward = self.model.feature_layers[0].nfe\n",
        "                self.model.feature_layers[0].nfe = 0\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                #print(\"[%d] Completed backward\" % batch_turn)\n",
        "                nfe_backward = self.model.feature_layers[0].nfe\n",
        "                self.model.feature_layers[0].nfe = 0\n",
        "                self.batch_time_meter.update(time.time() - end)\n",
        "                self.f_nfe_meter.update(nfe_forward)\n",
        "                self.b_nfe_meter.update(nfe_backward)\n",
        "                end = time.time()\n",
        "                print(\"[%d] Completed one batch\" % batch_turn)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                train_acc = self.accuracy(self.model, test_dataloader)\n",
        "                val_acc = self.accuracy(self.model, val_dataloader)\n",
        "                print(\n",
        "                     \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | Train Acc {:.4f} | Test Acc {:.4f}\".format( itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg, b_nfe_meter.avg, train_acc, val_acc )\n",
        "                     )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um68hHB8yabg",
        "colab_type": "text"
      },
      "source": [
        "#Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV-Q0qxP52qZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader, test_dataloader, val_dataloader = train_val_test_split(batch_size=BATCH_SIZE, validation_ratio=0.1, test_ratio=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvmn0nFByljF",
        "colab_type": "text"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5rkG4aeJol2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ODEModel(9) #method is None\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
        "criterion = nn.MSELoss().to(DEVICE)\n",
        "batches_per_epoch = train_dataloader.size // train_dataloader.batchsize\n",
        "lr_fn = learning_rate_with_decay(\n",
        "        BATCH_SIZE, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n",
        "        decay_rates=[1, 0.1, 0.01, 0.001]\n",
        ")\n",
        "trainer = Trainer(model=model,optimizer=optimizer,criterion=criterion,\n",
        "                  accuracy=accuracy,lr_fn=lr_fn,meter_fn=RunningAverageMeter())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV4dGU-o53u6",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb3aCXUKKurg",
        "colab_type": "code",
        "outputId": "371c7ebc-59ab-4a37-9261-320d99c6824c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "trainer.run(NEPOCHS, train_dataloader, val_dataloader, test_dataloader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] Completed one batch\n",
            "[1] Completed one batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clp_P7X85w9t",
        "colab_type": "text"
      },
      "source": [
        "# Predict input\n",
        "\n",
        "adjoint.py의 81번째 코드 참고!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njYhQpNy5v-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict x by y\n",
        "search_size = 1\n",
        "for itr in range(min(train_size, search_size)):\n",
        "    x,y = train_dataloader.next(1)\n",
        "    y = np.transpose(y.numpy())\n",
        "\n",
        "    # Linear\n",
        "    weight = model[5].weight\n",
        "    bias = model[5].bias.cpu().detach().numpy()\n",
        "    bias = bias.reshape((2,1))\n",
        "    z_tn = np.subtract(y, bias)\n",
        "    inverse_weight = np.linalg.pinv(weight.cpu().detach().numpy())\n",
        "    z_tn = np.matmul(inverse_weight, z_tn)\n",
        "\n",
        "    # AdaptiveAvgPool1d(1)\n",
        "    z_tn = np.repeat(z_tn, EMBED_SIZE, axis=1)\n",
        "    z_tn = np.expand_dims(z_tn, axis=0)\n",
        "\n",
        "    # ode back\n",
        "    z_tn = torch.from_numpy(z_tn).to(DEVICE)\n",
        "    z_t0 = odeint(ODEf, z_tn, torch.tensor([1, 0]).float().type_as(z_tn) , rtol=RTOL, atol=ATOL, method = 'adams')[1]\n",
        "    # output[0] is for time t0, output[1] is for time t1.\n",
        "\n",
        "    assert z_t0.shape == (1, 9,18)\n",
        "\n",
        "    z_t0 = z_t0.cpu().detach().numpy()\n",
        "    encoded = max_index(z_t0)\n",
        "    \n",
        "    print(\"encoded\")\n",
        "    for enc_ in encoded:\n",
        "        print(enc_)\n",
        "    \n",
        "    mat, op = decode(encoded)\n",
        "\n",
        "    print(\"mat\")\n",
        "    for mat_ in mat:\n",
        "        print(mat_)\n",
        "    print(\"y:\", y)\n",
        "\n",
        "\n",
        "    def __run_experiment(mat, op):\n",
        "        model_spec = api.ModelSpec(matrix=mat, ops=op)\n",
        "        q = nasbench_api.query(model_spec)\n",
        "        return q['validation_accuracy'], q['training_time']\n",
        "\n",
        "\n",
        "    print(\"result\", __run_experiment(mat, op))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}